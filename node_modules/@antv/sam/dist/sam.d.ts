import { InferenceSession, Tensor } from 'onnxruntime-web';
import { modelInputProps } from './api/onnxModel';
import { IHandleScale } from './utils/scale';
export interface ISAMOptions {
    modelUrl?: string;
    wasmPaths?: string;
}
export declare class SAM {
    private modelUrl;
    model: InferenceSession;
    image: HTMLImageElement;
    imageData: ImageData | undefined;
    modelScale: IHandleScale;
    tensor: Tensor;
    constructor(options: ISAMOptions);
    initModel(): Promise<void>;
    setImage(image: HTMLImageElement | string): Promise<void>;
    setEmbedding(tensorFile: ArrayBuffer | string, dType?: string): Promise<void>;
    /**
       *  document https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb
       *
       *  the ONNX model has a different input signature than SamPredictor.predict. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are np.float32.
          image_embeddings: The image embedding from predictor.get_image_embedding(). Has a batch index of length 1.
          point_coords: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. Coordinates must already be transformed to long-side 1024. Has a batch index of length 1.
          point_labels: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.
          mask_input: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.
          has_mask_input: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.
          orig_im_size: The size of the input image in (H,W) format, before any transformation.
          Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at sam.mask_threshold (equal to 0.0).
  
       */
    predict(points: Array<modelInputProps>): Promise<Tensor | undefined>;
    predictByBox(box: Array<number>): Promise<number[]>;
    exportMaskImage(output: any): HTMLImageElement | undefined;
    exportMaskClip(output: any): HTMLImageElement;
    /**
     *
     * @param output
     * @param flag 是否反转mask
     * @returns
     */
    exportImage(output: any, flag?: boolean): HTMLImageElement | undefined;
    exportImageClip(output: any): HTMLImageElement | undefined;
    /**
     *
     * @param output
     * @param simplifyThreshold 抽稀系数
     * @returns
     */
    exportVector(output: any, simplifyThreshold?: number): any;
    setWasmUrl(url: string): void;
    private getImageScale;
}
